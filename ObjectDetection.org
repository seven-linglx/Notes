#+TITLE: Object Detection
#+STARTUP: show1levels
#+STARTUP: INDENT
#+AUTHOR: lingluxiang
#+DATA: <2021-07-18>

* 目标检测概述
** ResNet
1) ResNet解决深度网络梯度回传消失
2) 增加所有牲征图的表达能力
** FPN
1) 将不同大小的特征图进行融合
2) feature pyramid network
3) 主网络采用ResNet
[[file:images/fpn.jpg]]
** RPN
1) Region Proposal Networks
2) 首先通过一系列卷积得到公共特征图，假设他的大小是N x 16 x 16，然后我们进入RPN阶段，首先经过一个3 x
   3的卷积，得到一个256 x 16 x 16的特征图，也可以看作16 x 16个256维特征向量，然后经过两次1 x 1的卷积，
   分别得到一个18 x 16 x 16的特征图，和一个36 x 16 x 16的特征图，也就是16 x 16 x 9个结果，每个结果包
   含2个分数和4个坐标，再结合预先定义的Anchors，经过后处理，就得到候选框；
3) 在最后一层feature map里,每个点预设K(9)个Anchors,每个Anchor有2个分类分数和4个框,这个思想可称为RPN
[[file:images/rpn.jpg]]
** RetinaNet
1) FocalLoss
** 感受野对于检测网络的意义
1) 分类网络性能提升带动检测网络性能提升
2) 输出层feature map里每个点对应一个类别标签,feature map里每个点是原图的一个区域(可以看成是原图的一个子图)
3) 目标检测网络本质训练一个分类网络,框的回归当成了附带的事情
** loss
+ [[https://blog.csdn.net/lt1103725556/article/details/115128523][交叉熵\FCOS\正负样本划分]]
** Tips
1) top-down下采样(max pooling)
2) down-top上采样(反卷积)
3) 检测网络中,习惯性地将整体网络结构划分为三个部分:backbone --> neck --> head
4) neck: 特征融合,典型代表FPN,也有可能不是[[https://arxiv.org/abs/2103.09460][You Only Look One-level Feature]]
*** backbone
1) 为目标检测提供若干种感受野大小和中心步长的组合,以满足对不同尺度和类别的目标检测
2) 把feature map大小不改变的层归成一个Stage
3) backbone描述成一系列的block(S, L, F): S代表相对输入图像的Stride, L代表有多少个3x3的Conv,F代表输出
   的Channel数目
*** head
1) 分类\回归\Quality
2) Quality有centerness和iou score评论方式
* 目标分割(知乎lighthouse)
1) 分割网络的设计重点是如何更加高效地耦合更多尺寸感受野的特征
* Anchor-based & Anchor-free(知乎lighthouse)
1) 基于一个point,Anchor-based事先画好几个固定的框,回归是基于此.Anchor-free不事先标记框,直接都这个点回归boundingbox
2) Anchor-based超参数多些,复杂度高,bbox回归相对简单下,一个point可以回归多个bbox
3) Anchor-free几乎没有超参数,复杂度低,一个point回归一个bbox.
4) CenterNet、CornerNet这类的方法不是基于point的，是预测框的角，然后再用后处理把相关的角联系起来。
* 感受野(知乎lighthouse)
1) 深度卷积神经网络中,每个神经元对应输入图像的某个确定区域,仅该区域对神经元激活起作用,这个区域叫这个神经元的感受野.
2) 有效感受野目前没有量化方式,只是一种现象
3) 感觉野做为anchor没有意义,有效感受野倒可以
4) 感受野的尺寸可能比原图大
5) 输出featuremap某个节点的响应对应的输入图像的区域就是感受野
6) pooling层，可将其当成特殊的卷积层，同样存在kernel size、padding、stride参数
file:images/rf.svg
[[https://www.cnblogs.com/shine-lee/p/12069176.html][感受野计算]]
* feature map
1) out = ((input_size + 2 * padding - kernel_size) / stride) + 1 [[https://zhuanlan.zhihu.com/p/29119239][(CNN中卷积层的计算细节]])
2) 卷积层向下取整,池化层向上取整
3) feature map里每个点是原图的一个区域(感受野)的中心
4) 最后一层feature map,有可能有多个点都能对应回原图里目标物
5) 一个point对应一个目标,怎么处理遮挡或重叠?
* Anchor原理(bilibili)
** label assignment 准备训练数据
- 把目标分配给某个Anchor
- 目标与Anchor计算IoU,取最大的
** 存在的地方
- 分类目标匹配
- Target回归
** anchor(锚点) box/point
- FCOS anchor free : 图大稀疏
- RetinaNet anchor based : 图小密集
* Anchor详解
** 概述
1) Anchor box通常是以CNN提取到的Feature Map 的点为中心位置，生成边框，所以一个Anchor box不需要指定中
  心位置
2) Anchor Box的生成是以CNN网络最后生成的Feature Map上的点为中心的（映射回原图的坐标), 降采样16倍, 最
  后一层feature map上的点对应原图16x16的区域
3) 使用scale(尺度)和ratio(长宽比)来描述,scale在基础尺寸上的缩放倍数
4) 注意scale的参照物,相对最后FeatureMap,还是相对原图
5) 坐标归一化:(x/w, y/h)(w=h)来表示点
6) faster-rcnn anchor计算源码:[[https://github.com/rbgirshick/py-faster-rcnn/blob/master/lib/rpn/generate_anchors.py][faster-rcnn]]
7) [[https://blog.csdn.net/hust_lmj/article/details/80152850][faster-rcnn anchor源码解析]]
** 中心
1) Feature map上(0,0)点，对应原图的(0,0,15,15)(左上角坐标，右下角坐标)，该点生成Anchor box的中心点就
  是原图的(7.5,7.5), Feature Map上其余位置在原图对应的中心点在此基础上进行平移即可得到。例如Feature
  Map上(0,0)的点在原图上对应区域的的中心点为(7.5,7.5),则(0,1)对应的中心点为(7.5,7.5 + 16)
** 长宽
1) width = scale * base_size / sqrt(ratio) # base_size = 16(faster-rcnn), 一个point对应的感受野边长
2) length = scale * base_size * sqrt(ratio) # ratio = length / width, scale = 8, 16, 32(faster-rcnn)
* label assignment
+ 前景分配cls_loss和reg_loss
+ 背景只分配cls_loss
+ 基于IoU的正负样本划分
  #+begin_quote
  满足以下条件的Anchor是正样本：
  与Ground Truth Box的IOU(Intersection-Over-Union) 的重叠区域最大的Anchor；
  与Gound Truth Box的IOU的重叠区域>0.7;

  满足以下条件的Anchor是负样本：
  与Gound Truth Box的IOU的重叠区域 <0.3;
  既不属于正样本又不属于负样本的Anchor不参与训练。
  #+end_quote
+ 在加载数据集时,就已经根据设计的网络输出和anchor信息,确定了数据GT矩阵大小,保持和网络输出同样尺寸

* 动态样本划分(bilibili)
** 静态Anchor匹配机制弊端
- 小目标: 漏检, 解决: 1) anchor大小聚类得出, 2) feature map
- 大目标: 误检
** 动态样本划分
- IoU的阈值使用统计量,所有与这个样本相关的Anchor求的IoU的mean + std
- PAPER: ATSS(Adaptive Training Sample Selection), PAA
* PointPillar
** pillarnet
+ pointcloud网络化,0.16m一个格子,变成H*W尺寸
+ 输入尺寸变化: P*N*D(30000 x 20 x 9)->P*N*C(30000 x 20 x 64)-> P*C(30000*64)->H*W*C(512*512*64),这
  里的H,W就是上面PointCloud的尺寸
** backbone
+ backbone网络结构,不是FPN结构,因为没有融合feature mpa,和SSD采用方式类似(FCN style)
[[file:images/pointpillars.backbone.png]]
+ 三个Block(每个尺寸减少一倍,通道增加一倍)
** neck
+ backbone里每个block的输出接一个上采样(反卷积),使得neck的输出有相同的尺寸与通道,最后三层一起Concat
** head
+ backbone最后一层feature map里每个点接出6个head做不同的预测(类别,Anchor位置,Anchor尺寸...),此做法和
  RPN一样,属于Anchor-Based
** loss
+ [[https://zhuanlan.zhihu.com/p/102994173][参考链接]]
+ reg_loss, cls_loss, dir_loss组成
+ 区分正负样本,用所有的样本计算cls_loss,再用正样本计算reg_loss,一般分类与回归是不同的HEAD
[[file:images/pointpillarsloss.svg]]
+ reg_loss
[[file:images/pointpillarslocloss.jpg]]
+ cls_loss使用focal loss,能调节正负样本不均衡\难易样本不均衡的分类训练
[[file:images/pointpillarsclsloss.jpg]]
[[file:images/focalloss.jpg]]
* SSD
** pytorch版本代码
+ https://github.com/amdegroot/ssd.pytorch
+ https://github.com/lufficc/SSD
* 参考
1) A Guide to Convolution Arithmetic for Deep Learning
